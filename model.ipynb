{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet ChatBot 2020\n",
    "## MOREL Louis - MONET Anaïs - SDIA - TPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from spacy.lang.fr import French\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"/Users/monetanais/Documents/Cours/3A/chatbot/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for filename in os.listdir(database_path):\n",
    "    if not filename.startswith('.'):\n",
    "        df = df.append(pd.read_excel(database_path + filename, names = ['attente','message']), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attente</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>impression</td>\n",
       "      <td>Bonjour, je voudrais imprimer le document toto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>impression</td>\n",
       "      <td>Je voudrais imprimer le document doc1 qui cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>impression</td>\n",
       "      <td>Bonjour, imprime moi doc2 qui contient 4500 pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>impression</td>\n",
       "      <td>Impression doc3 4500 pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>impression</td>\n",
       "      <td>tirage mon document doc4 à 1800 pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>autre</td>\n",
       "      <td>pouvez-vous me dire pourquoi l'école est fermée ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>autre</td>\n",
       "      <td>5 5 5 doc documents pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>autre</td>\n",
       "      <td>pages doc impression 23 tata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>autre</td>\n",
       "      <td>Imprimes encore 1 fois mon document, je ne me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>autre</td>\n",
       "      <td>je n'aime pas windows</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        attente                                            message\n",
       "0    impression  Bonjour, je voudrais imprimer le document toto...\n",
       "1    impression  Je voudrais imprimer le document doc1 qui cont...\n",
       "2    impression  Bonjour, imprime moi doc2 qui contient 4500 pages\n",
       "3    impression                         Impression doc3 4500 pages\n",
       "4    impression              tirage mon document doc4 à 1800 pages\n",
       "..          ...                                                ...\n",
       "394       autre  pouvez-vous me dire pourquoi l'école est fermée ?\n",
       "395       autre                          5 5 5 doc documents pages\n",
       "396       autre                       pages doc impression 23 tata\n",
       "397       autre  Imprimes encore 1 fois mon document, je ne me ...\n",
       "398       autre                             je n'aime pas windows \n",
       "\n",
       "[398 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage de l'attente\n",
    "Pour pouvoir créer notre outil de classification, nous allons séparer nos données en deux parties : \n",
    "- **`X`** est utilisé pour les données sources utilisées pour l'apprentissage, c'est à dire l'ensemble des traits ici les messages reçus\n",
    "- **`Y`** est utilisé pour ce que l'on cherche à prédire : impression ou autre\n",
    "- **`train`** données d'apprentissage \n",
    "- **`test`** données d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "X = pd.DataFrame(X)\n",
    "y = df['attente']\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message\n",
      "0  Bonjour, je voudrais imprimer le document toto...\n",
      "1  Je voudrais imprimer le document doc1 qui cont...\n",
      "2  Bonjour, imprime moi doc2 qui contient 4500 pages\n",
      "3                         Impression doc3 4500 pages\n",
      "4              tirage mon document doc4 à 1800 pages\n",
      "      attente\n",
      "0  impression\n",
      "1  impression\n",
      "2  impression\n",
      "3  impression\n",
      "4  impression\n"
     ]
    }
   ],
   "source": [
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réaliser nos premières expériences, nous allons découper automatiquement les données en jeu d'apprentissage et de test. Ici, 80% des données seront utilisées pour l'apprentissage, 20% pour les tests :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement de la colonne **`message`**. Transformation du **`message`** en sac de mots.\n",
    "Nous allons dans un premier temps définir deux classes qui nous seront utiles dans les pipelines : la classe `SingleColumnSelector` qui nous permet de sélectionner une colonne de la table de données d'entrée à partir de son nom, et la classe `MultiColumnSelector` qui permet de sélectionner plusieurs colonnes de la table de données d'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "    \n",
    "class MultiColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "        try:\n",
    "            return X[self.columns].to_dict('records')\n",
    "        except KeyError:\n",
    "            cols_error = list(set(self.columns) - set(X.columns))\n",
    "            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokénisation\n",
    "On va transformer le texte contenu dans message en vecteur. On va découper selon les espaces entre les mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "266    merci de débuter l'impression du document 8 qu...\n",
      "326    j'ai besoin d'imprimer 2 fois le doc 1 et ses ...\n",
      "367    Bonjour j'aurais besoin d'imprimer doc8 faisan...\n",
      "318                      tirage : 78 pages, document 34 \n",
      "375                   il fait vraiment beau en ce moment\n",
      "Name: message, dtype: object\n",
      "\n",
      "Output     Occurences\n",
      "  (0, 44)\t1\n",
      "  (0, 74)\t1\n",
      "  (0, 182)\t1\n",
      "  (0, 219)\t1\n",
      "  (0, 230)\t1\n",
      "  (0, 232)\t1\n",
      "  (0, 264)\t1\n",
      "  (0, 320)\t1\n",
      "  (0, 354)\t1\n",
      "  (0, 388)\t1\n",
      "  (0, 452)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 30)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 116)\t1\n",
      "  (1, 177)\t1\n",
      "  (1, 199)\t1\n",
      "  (1, 256)\t1\n",
      "  (1, 273)\t1\n",
      "  (1, 308)\t1\n",
      "  (1, 332)\t1\n",
      "  (1, 388)\t1\n",
      "  (1, 483)\t1\n",
      "  (2, 49)\t1\n",
      "  (2, 116)\t1\n",
      "  (2, 122)\t1\n",
      "  (2, 177)\t1\n",
      "  (2, 209)\t1\n",
      "  (2, 263)\t1\n",
      "  (2, 354)\t1\n",
      "  (2, 388)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 42)\t1\n",
      "  (3, 69)\t1\n",
      "  (3, 87)\t1\n",
      "  (3, 219)\t1\n",
      "  (3, 389)\t1\n",
      "  (3, 512)\t1\n",
      "  (4, 135)\t1\n",
      "  (4, 246)\t1\n",
      "  (4, 264)\t1\n",
      "  (4, 287)\t1\n",
      "  (4, 547)\t1\n",
      "\n",
      "0 \n",
      "1 !\n",
      "2 (doc2)\n",
      "3 ,\n",
      "4 ,234\n",
      "5 1\n",
      "6 100\n",
      "7 12\n",
      "8 122\n",
      "9 123\n",
      "10 1230\n",
      "11 1234\n",
      "12 124\n",
      "13 127\n",
      "14 128\n",
      "15 13\n",
      "16 132\n",
      "17 134\n",
      "18 14\n",
      "19 15\n"
     ]
    }
   ],
   "source": [
    "# Fonction de tokénisation\n",
    "def tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "# Objet CountVectorizer pour la transformation en sac de mots\n",
    "var_vectorizer = CountVectorizer(tokenizer = tokenize, min_df = 0.001)\n",
    "    \n",
    "# Pipeline spécifique\n",
    "message_pipeline = make_pipeline(\n",
    "    # Sélection de la colonne \"message\"\n",
    "    SingleColumnSelector(key = 'message'),\n",
    "    # Transformation sac de mots\n",
    "    var_vectorizer\n",
    ")\n",
    "\n",
    "# Exemple d'application de la pipeline\n",
    "# Apprentissage du vocabulaire à l'aide de \"fit\"\n",
    "message_pipeline.fit(X_train)\n",
    "# Transformation en sac de mots à l'aide de \"transform\"\n",
    "res = message_pipeline.transform(X_test.head())\n",
    "print(\"Input\")\n",
    "print(X_test.message.head())\n",
    "print()\n",
    "print(\"Output     Occurences\")\n",
    "print(res)\n",
    "print()\n",
    "\n",
    "# Affichage des traits extraits par la pipeline\n",
    "feat_names = var_vectorizer.get_feature_names()\n",
    "for i in range(20) :\n",
    "    print(i, feat_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = French()\n",
    "def split_into_lemmas_spacy(desc) :\n",
    "    doc = nlp(desc)\n",
    "    return [w.lemma_ for w in doc]\n",
    "\n",
    "nltk_stopwords = stopwords.words('french')+list(string.punctuation)\n",
    "#print(nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "merci de débuter l'impression du document 8 qui fait 345 pages\n",
      "\n",
      "Output\n",
      "  (0, 313)\t0.127438375036568\n",
      "  (0, 296)\t0.36242349413885805\n",
      "  (0, 276)\t0.30952095214039715\n",
      "  (0, 253)\t0.22624765224992763\n",
      "  (0, 230)\t0.27453485130570826\n",
      "  (0, 201)\t0.4889483553921806\n",
      "  (0, 191)\t0.15858889590029673\n",
      "  (0, 67)\t0.35602898833656654\n",
      "  (0, 41)\t0.4889483553921806\n",
      "  (1, 313)\t0.12075898406759726\n",
      "  (1, 272)\t0.23328598085833516\n",
      "  (1, 257)\t0.21702717098664723\n",
      "  (1, 238)\t0.4397992545470888\n",
      "  (1, 173)\t0.33172742103991676\n",
      "  (1, 155)\t0.28996035628189465\n",
      "  (1, 106)\t0.3373685431707782\n",
      "  (1, 30)\t0.35708687908742565\n",
      "  (1, 27)\t0.35708687908742565\n",
      "  (1, 2)\t0.35708687908742565\n",
      "  (2, 313)\t0.12276941932202443\n",
      "  (2, 296)\t0.3491453960498098\n",
      "  (2, 272)\t0.23716980253754605\n",
      "  (2, 257)\t0.22064031065562548\n",
      "  (2, 229)\t0.4471211770776503\n",
      "  (2, 182)\t0.3133365352817221\n",
      "  (2, 155)\t0.29478771158929823\n",
      "  (2, 112)\t0.322094635695424\n",
      "  (2, 106)\t0.3429851655542987\n",
      "  (2, 46)\t0.3895032680258982\n",
      "  (3, 409)\t0.4055732295754736\n",
      "  (3, 313)\t0.1994016854010037\n",
      "  (3, 191)\t0.2481426267349173\n",
      "  (3, 63)\t0.577887482540196\n",
      "  (3, 39)\t0.6326299215429266\n",
      "  (4, 434)\t0.8857669062766771\n",
      "  (4, 230)\t0.4641303564140622\n",
      "\n",
      "313 pages\n",
      "296 merci\n",
      "276 l'\n",
      "253 impression\n",
      "230 fait\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monetanais/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# Objet TfidfVectorizer\n",
    "msg_vectorizer = TfidfVectorizer(tokenizer=split_into_lemmas_spacy, \n",
    "                                lowercase=True, \n",
    "                                stop_words=nltk_stopwords, \n",
    "                                min_df=0.001)\n",
    "\n",
    "# Pipeline spécifique\n",
    "msg_pipeline = make_pipeline(\n",
    "    SingleColumnSelector(key=\"message\"),\n",
    "    msg_vectorizer\n",
    ")\n",
    "\n",
    "# Exemple d'application de la pipeline\n",
    "msg_pipeline.fit(X_train)\n",
    "res = msg_pipeline.transform(X_test.head())\n",
    "print(\"Input\")\n",
    "print(X_test.message.iloc[0])\n",
    "print()\n",
    "print(\"Output\")\n",
    "print(res)\n",
    "print()\n",
    "\n",
    "# Affichage des traits extraits par la pipeline\n",
    "feat_names = msg_vectorizer.get_feature_names()\n",
    "print(313, feat_names[313])\n",
    "print(296, feat_names[296])\n",
    "print(276, feat_names[276])\n",
    "print(253, feat_names[253])\n",
    "print(230, feat_names[230])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire des stats sur l'occurence des mots dans message ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318, 444)\n",
      "# msg features :  444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monetanais/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# Union des traits\n",
    "union = FeatureUnion(transformer_list = [\n",
    "        (\"msg_feature\", msg_pipeline)\n",
    "    ])\n",
    "\n",
    "# Chaîne de prétraitement globale, composée de l'union des chaînes\n",
    "preprocess_pipeline = make_pipeline(\n",
    "    union\n",
    ")\n",
    "\n",
    "# Application de la chaîne à X_train\n",
    "preprocess_pipeline.fit(X_train)\n",
    "X_transformed = preprocess_pipeline.transform(X_train)\n",
    "print(X_transformed.shape)\n",
    "\n",
    "#Affichage du nombre de traits générés par chacune des chaines \n",
    "fnames_msg = msg_pipeline.named_steps['tfidfvectorizer'].get_feature_names()\n",
    "print('# msg features : ', len(fnames_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               message\n",
      "266  merci de débuter l'impression du document 8 qu...\n",
      "326  j'ai besoin d'imprimer 2 fois le doc 1 et ses ...\n",
      "367  Bonjour j'aurais besoin d'imprimer doc8 faisan...\n",
      "318                    tirage : 78 pages, document 34 \n",
      "375                 il fait vraiment beau en ce moment\n",
      "95   j'aimerais que tu me tires mon document doc0 q...\n",
      "114       quelle est la météo de demain a Strasbourg ?\n",
      "262  J'ai un document très long à imprimer. J'aimer...\n",
      "42    Bonjour, tire moi les 132 pages du document doc6\n",
      "159  où en est l'impression de mon document doc7 de...\n",
      "239  Lances moi l'impression du doc6 qui fait 56 pa...\n",
      "13         Bonjour, imprime le document toto à 3 pages\n",
      "54                    tire doc8  qui contient 67 pages\n",
      "258      Pourriez-vous m'imprimer le doc8 avec 3 pages\n",
      "260                    J'ai besoin des 9 pages du doc1\n",
      "276  commences par imprimer le document 65 et ses 8...\n",
      "289                      imprimez document 8 34pages !\n",
      "23                 tirage du document toto à 123 pages\n",
      "136                   je préfère le rap comme musique \n",
      "374                      toto 98 document pages tirage\n",
      "\n",
      "Predicted :\n",
      "['impression' 'impression' 'impression' 'impression' 'autre' 'impression'\n",
      " 'autre' 'impression' 'impression' 'impression' 'impression' 'impression'\n",
      " 'impression' 'impression' 'impression' 'impression' 'autre' 'impression'\n",
      " 'autre' 'impression']\n",
      "\n",
      "Expected : \n",
      "        attente\n",
      "266  impression\n",
      "326       autre\n",
      "367  impression\n",
      "318  impression\n",
      "375       autre\n",
      "95   impression\n",
      "114       autre\n",
      "262  impression\n",
      "42   impression\n",
      "159       autre\n",
      "239  impression\n",
      "13   impression\n",
      "54   impression\n",
      "258  impression\n",
      "260  impression\n",
      "276  impression\n",
      "289  impression\n",
      "23   impression\n",
      "136       autre\n",
      "374       autre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monetanais/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Prétaitement + apprentissage\n",
    "# LogisticRegression() 0.9\n",
    "# MultinomailNB() 0.8375\n",
    "# DecisionTreeClassifier() 0.8375\n",
    "# RandomForestClassifier() 0.8875\n",
    "# Si on veut utiliser KNneighborsClassifier il faut trouver un moyen pour déterminer k\n",
    "classifier_pipeline = make_pipeline(\n",
    "    preprocess_pipeline,\n",
    "    LogisticRegression()\n",
    ")\n",
    "# Apprentissage avec les données d'entraînement\n",
    "classifier_pipeline.fit(X_train, y_train)\n",
    "# Test sur des données issues du jeu de test (uniquement les premières lignes)\n",
    "predicted = classifier_pipeline.predict(X_test.head(20))\n",
    "print(X_test.head(20))\n",
    "print()\n",
    "print(\"Predicted :\")\n",
    "print(predicted)\n",
    "print()\n",
    "print(\"Expected : \")\n",
    "print(y_test.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['impression' 'impression' 'impression' 'impression' 'autre' 'impression'\n",
      " 'autre' 'impression' 'impression' 'impression' 'impression' 'impression'\n",
      " 'impression' 'impression' 'impression' 'impression' 'autre' 'impression'\n",
      " 'autre' 'impression' 'autre' 'impression' 'autre' 'autre' 'impression'\n",
      " 'impression' 'impression' 'impression' 'autre' 'autre' 'impression'\n",
      " 'impression' 'impression' 'impression' 'impression' 'impression' 'autre'\n",
      " 'autre' 'autre' 'autre' 'impression' 'impression' 'autre' 'autre'\n",
      " 'impression' 'impression' 'impression' 'autre' 'impression' 'impression'\n",
      " 'impression' 'impression' 'impression' 'impression' 'impression'\n",
      " 'impression' 'autre' 'impression' 'impression' 'impression' 'impression'\n",
      " 'impression' 'impression' 'impression' 'impression' 'autre' 'autre'\n",
      " 'autre' 'impression' 'impression' 'impression' 'autre' 'impression'\n",
      " 'autre' 'autre' 'autre' 'autre' 'impression' 'impression' 'impression']\n"
     ]
    }
   ],
   "source": [
    "all_predictions = classifier_pipeline.predict(X_test)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9\n",
      "confusion matrix\n",
      "             autre  impression\n",
      "autre          23           6\n",
      "impression      2          49\n",
      "(row=expected, col=predicted)\n"
     ]
    }
   ],
   "source": [
    "print('accuracy', accuracy_score(y_test, all_predictions))\n",
    "labels = np.unique(y_test)\n",
    "cm =  confusion_matrix(y_test, all_predictions, labels=labels)\n",
    "confusion_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print('confusion matrix\\n', confusion_df)\n",
    "print('(row=expected, col=predicted)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Quelle heure est-il ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 message\n",
       "0  Quelle heure est-il ?"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame({'message' : ['Quelle heure est-il ?']})\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['autre'], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_pipeline.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
